
"""
Benchmark Runner

Main script for running all benchmarks and generating reports.
"""

import sys
import argparse
from pathlib import Path
from datetime import datetime

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from core.config import Config
from benchmarks.agent_benchmarks import AgentBenchmarks
from benchmarks.llm_benchmarks import LLMBenchmarks
from benchmarks.memory_benchmarks import MemoryBenchmarks
from benchmarks.orchestration_benchmarks import OrchestrationBenchmarks


class BenchmarkRunner:
    """Run all benchmarks and generate reports."""
    
    def __init__(self, config: Config):
        """
        Initialize benchmark runner.
        
        Args:
            config: System configuration
        """
        self.config = config
        self.results = {}
    
    def run_all(self):
        """Run all benchmark suites."""
        print("="*80)
        print("AI Agent Console - Performance Benchmarks")
        print("="*80)
        print()
        
        # Agent benchmarks
        print("\n--- Agent Execution Benchmarks ---")
        agent_bench = AgentBenchmarks(self.config)
        self.results["agents"] = agent_bench.run_all_benchmarks()
        
        # LLM benchmarks
        print("\n--- LLM Query Benchmarks ---")
        llm_bench = LLMBenchmarks(self.config)
        self.results["llm"] = llm_bench.run_all_benchmarks()
        
        # Memory benchmarks
        print("\n--- Memory System Benchmarks ---")
        memory_bench = MemoryBenchmarks(self.config)
        self.results["memory"] = memory_bench.run_all_benchmarks()
        
        # Orchestration benchmarks
        print("\n--- Orchestration System Benchmarks ---")
        orch_bench = OrchestrationBenchmarks(self.config)
        self.results["orchestration"] = orch_bench.run_all_benchmarks()
        
        print("\n" + "="*80)
        print("All benchmarks completed!")
        print("="*80)
    
    def generate_report(self) -> str:
        """
        Generate comprehensive benchmark report.
        
        Returns:
            Markdown formatted report
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report = f"""# AI Agent Console - Performance Benchmarks

**Date:** {timestamp}  
**Version:** 2.4.0

This report contains performance benchmarks for the AI Agent Console system.

---

## Summary

- **Agent Benchmarks:** {len(self.results.get('agents', []))} tests
- **LLM Benchmarks:** {len(self.results.get('llm', []))} tests
- **Memory Benchmarks:** {len(self.results.get('memory', []))} tests
- **Orchestration Benchmarks:** {len(self.results.get('orchestration', []))} tests

---

"""
        
        # Add individual reports
        if "agents" in self.results:
            agent_bench = AgentBenchmarks(self.config)
            agent_bench.results = self.results["agents"]
            report += agent_bench.generate_report() + "\n---\n\n"
        
        if "llm" in self.results:
            llm_bench = LLMBenchmarks(self.config)
            llm_bench.results = self.results["llm"]
            report += llm_bench.generate_report() + "\n---\n\n"
        
        if "memory" in self.results:
            memory_bench = MemoryBenchmarks(self.config)
            memory_bench.results = self.results["memory"]
            report += memory_bench.generate_report() + "\n---\n\n"
        
        if "orchestration" in self.results:
            orch_bench = OrchestrationBenchmarks(self.config)
            orch_bench.results = self.results["orchestration"]
            report += orch_bench.generate_report() + "\n---\n\n"
        
        # Add recommendations
        report += """## Recommendations

### Performance Optimization Tips

1. **Agent Execution**
   - Cache frequently used LLM responses
   - Optimize agent selection logic
   - Use parallel execution when possible

2. **LLM Queries**
   - Use appropriate models for tasks (smaller for simple tasks)
   - Implement request batching
   - Cache repeated queries

3. **Memory Operations**
   - Optimize vector database indices
   - Implement memory pruning
   - Use appropriate collection sizes

4. **Orchestration**
   - Minimize task decomposition overhead
   - Cache agent capabilities
   - Optimize workflow execution

---

## System Information

- **Python Version:** {sys.version.split()[0]}
- **Config File:** config.yaml
- **Database:** ChromaDB (vector memory)

---

*Generated by AI Agent Console Benchmark Suite v1.0.0*
"""
        
        return report
    
    def save_report(self, output_path: Path):
        """
        Save benchmark report to file.
        
        Args:
            output_path: Path to save report
        """
        report = self.generate_report()
        output_path.write_text(report)
        print(f"\nReport saved to: {output_path}")


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Run AI Agent Console benchmarks")
    parser.add_argument(
        "--output",
        "-o",
        type=Path,
        default=Path("benchmark_report.md"),
        help="Output path for benchmark report"
    )
    parser.add_argument(
        "--config",
        "-c",
        type=Path,
        default=Path("config.yaml"),
        help="Path to configuration file"
    )
    
    args = parser.parse_args()
    
    # Load config
    config = Config(str(args.config))
    
    # Run benchmarks
    runner = BenchmarkRunner(config)
    runner.run_all()
    
    # Generate and save report
    runner.save_report(args.output)
    
    print(f"\n✓ Benchmarks completed successfully!")
    print(f"✓ Report saved to: {args.output}")


if __name__ == "__main__":
    main()
